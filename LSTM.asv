clc
clear all
close all

data = load("data.mat");
y = data.y';
u = data.u';

N_reg = 1;
% Inicializar los regresores como una matriz vacía
Z = [];

% Y
for i=1:N_reg % índice del regresor
    y_i = y(N_reg+1-i:end-i); % Toma los regresores como ventana deslizante 2 al final-2 y 1 al final-1
    Z = [Z y_i]; % Concatena el vector con los regresores
end

% U
% Se realiza el mismo procedimiento para u
for i=1:N_reg
    u_i = u(N_reg+1-i:end-i);
    Z = [Z u_i];
end


% Target
Y = y(N_reg+1:end);

% Split Train, Test, Val with random window sizes
n_data = length(Z);
n_train = ceil(n_data*0.6);
n_test = ceil(n_data*0.85);
%% Normal sequence

Z_train = Z(1:n_train,:);
Z_test = Z(n_train+1:n_test,:);
Z_val = Z(n_test+1:end,:);

Y_train = Y(1:n_train,:);
Y_test = Y(n_train+1:n_test,:);
Y_val = Y(n_test+1:end,:);

max_z = max(Z_train);
min_z = min(Z_train);
max_y = max(Y_train);
min_y = min(Y_train);

Z_ntrain = (Z_train - min_z) ./ (max_z-min_z);
Z_ntest = (Z_test - min_z) ./ (max_z-min_z);
Z_nval = (Z_val - min_z) ./ (max_z-min_z);

Y_ntrain = (Y_train - min_z) ./ (max_z-min_z);
Y_ntest = (Y_test - min_z) ./ (max_z-min_z);
Y_nval = (Y_val - min_z) ./ (max_z-min_z);
%% Building sequences try
min_window_size = 50; % Minimum window size
max_window_size = 200; % Maximum window size

% Initialize empty cell arrays for Train, Test, Val
Z_train = {};
Y_train = {};
Z_test = {};
Y_test = {};
Z_val = {};
Y_val = {};

% Generate random window sizes within the specified range
rand_window_sizes_train = randi([min_window_size, max_window_size], 1, ceil(n_train / min_window_size));
rand_window_sizes_test = randi([min_window_size, max_window_size], 1, ceil((n_test - n_train) / min_window_size));
rand_window_sizes_val = randi([min_window_size, max_window_size], 1, ceil((n_data - n_test) / min_window_size));

% Generate random starting points for each subset
rand_starts_train = sort(randi([1, n_train], 1, length(rand_window_sizes_train)));
rand_starts_test = sort(randi([n_train+1, n_test], 1, length(rand_window_sizes_test)));
rand_starts_val = sort(randi([n_test+1, n_data], 1, length(rand_window_sizes_val)));

% Extract subsets based on random lengths and starting points
for i = 1:length(rand_starts_train)
    start_idx = rand_starts_train(i);
    end_idx = min(start_idx + rand_window_sizes_train(i) - 1, n_train);
    Z_train{end+1} = Z(start_idx:end_idx, :);
    Y_train{end+1} = Y(start_idx:end_idx, :);
end

for i = 1:length(rand_starts_test)
    start_idx = rand_starts_test(i);
    end_idx = min(start_idx + rand_window_sizes_test(i) - 1, n_test);
    Z_test{end+1} = Z(start_idx:end_idx, :);
    Y_test{end+1} = Y(start_idx:end_idx, :);
end

for i = 1:length(rand_starts_val)
    start_idx = rand_starts_val(i);
    end_idx = min(start_idx + rand_window_sizes_val(i) - 1, n_data);
    Z_val{end+1} = Z(start_idx:end_idx, :);
    Y_val{end+1} = Y(start_idx:end_idx, :);
end

sequenceLengths = zeros(size(Z_train,1));
for i=1:numel(Z_train)
    sequence = Z_train{i};
    sequenceLengths(i) = size(sequence,1);
end

[sequenceLengths,idx] = sort(sequenceLengths,"descend");
Z_train = Z_train(idx);
Y_train = Y_train(idx);

figure
bar(sequenceLengths)
xlabel("Sequence")
ylabel("Length")
title("Sorted Data")

%%
% Calculate global max and min from the training data
global_mean_z = mean(cellfun(@(x) mean(x(:)), Z_train));
global_std_z = mean(cellfun(@(x) std(x(:)), Z_train));

global_mean_y = mean(cellfun(@(x) mean(x(:)), Y_train));
global_std_y = mean(cellfun(@(x) std(x(:)), Y_train));

% Normalize training data
Z_ntrain = cell(size(Z_train));
Y_ntrain = cell(size(Y_train));

for i = 1:length(Z_train)
    Z_ntrain{i} = (Z_train{i} - global_mean_z) ./ (global_std_z);
    Y_ntrain{i} = (Y_train{i} - global_mean_y) ./ (global_std_y);
end

% Normalize testing data based on training data min and max
Z_ntest = cell(size(Z_test));
Y_ntest = cell(size(Y_test));

for i = 1:length(Z_test)
    Z_ntest{i} = (Z_test{i} - global_mean_z) ./ (global_std_z);
    Y_ntest{i} = (Y_test{i} - global_mean_y) ./ (global_std_y);
end

% Normalize validation data based on training data min and max
Z_nval = cell(size(Z_val));
Y_nval = cell(size(Y_val));

for i = 1:length(Z_val)
    Z_nval{i} = (Z_val{i} - global_mean_z) ./ (global_std_z);
    Y_nval{i} = (Y_val{i} - global_mean_y) ./ (global_std_y);
end



%%
    % Layer Array Neural Network Architecture
    numFeatures = size(Z_ntrain,2); % ?
    numHiddenUnits = 20;
    numResponses = size(Y_ntrain,2); % ?
%%
    numFeatures = size(Z_ntrain{1},2); % ?
    numHiddenUnits = 40;
    numResponses = size(Y_ntrain{1},2); % ?
%%
    layers = [ ...
        sequenceInputLayer(numFeatures)
        lstmLayer(numHiddenUnits,OutputMode="sequence")
        fullyConnectedLayer(30)
        dropoutLayer(0.5)
        fullyConnectedLayer(numResponses)];
    
    % The training data must be sequences of different sizes
     


    % Training
    options = trainingOptions("adam", ...
        MaxEpochs=100, ...
        MiniBatchSize=50,...
        InitialLearnRate=0.001, ...
        SequenceLength='shortest', ...
        GradientThreshold=1,...
        Plots="training-progress", ...
        Shuffle="never", ...
        Verbose=0, ...
        ValidationFrequency = 10, ...  % a 1/8 of epochs needed for full test
        ValidationData={Z_ntest,Y_ntest}, ...
        LearnRateSchedule='piecewise', ... % Use piecewise learning rate schedule
        LearnRateDropFactor= 0.2, ... % Factor to drop learning rate
        LearnRateDropPeriod= 50)  % Every time it fully goes over train set
    

    lstm_model = trainnet(Z_ntrain,Y_ntrain,layers,'mse',options);
    save lstm_model;